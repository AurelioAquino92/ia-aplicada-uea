{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQSmBmLSfV_o"
      },
      "source": [
        "# Atividade Prática 2.2 - CIFAR 10\n",
        "\n",
        "* Disciplina _Inteligência Artificial Aplicada_\n",
        "* Professora: Elloá B. Guedes (ebgcosta@uea.edu.br)\n",
        "* Data de apresentação: 26 de janeiro de 2024\n",
        "* Data limite de entrega: 01 de fevereiro de 2024\n",
        "\n",
        "\n",
        "## Equipe\n",
        "* Integrante 1: *seu nome aqui*\n",
        "* Integrante 2: *seu nome aqui*\n",
        "\n",
        "\n",
        "## Contexto: Classificação de Imagens\n",
        "\n",
        "O CIFAR-10 é um conjunto de dados amplamente utilizado na comunidade de aprendizado de máquina e visão computacional para fins de treinamento e avaliação de algoritmos de classificação de imagem. O nome \"CIFAR\" é uma abreviação de \"Canadian Institute for Advanced Research\" (Instituto Canadense de Pesquisa Avançada), que é a organização que inicialmente coletou e disponibilizou esse conjunto de dados.\n",
        "\n",
        "## Base de Dados\n",
        "\n",
        "Disponível em: https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI7ujTRdfLhh"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH_4IWDsfVxT"
      },
      "outputs": [],
      "source": [
        "## Abrir a base de dados\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iby-6xhBhSZk"
      },
      "source": [
        "## Análise exploratória\n",
        "\n",
        "1. Quantos exemplos há no conjunto de treino?\n",
        "2. Quantos exemplos há no conjunto de teste?\n",
        "3. Imprima o exemplo 42 do conjunto de treinamento.\n",
        "4. Quantas classes há na tarefa? Qual a nomenclatura?\n",
        "5. Imprima uma amostra de exemplos do conjunto de treinamento\n",
        "6. As classes estão balanceadas no treinamento?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtvLOWSWhPlq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUpTmdSQhqYW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-b-wE7whr5p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySjQSX3Rh0c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHGCXeP9QKPl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD8FRiZvh5jM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(9):\n",
        " plt.subplot(330 + 1 + i)\n",
        " plt.imshow(X_train[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD3ko4Ja8KBz"
      },
      "source": [
        "## Normalização dos dados de treinamento\n",
        "\n",
        "Para uma melhor performance dos modelos de Redes Neurais no treinamento em bases de dados grandes, o ideal é que os valores de entrada estejam no intervalo [0,1]\n",
        "\n",
        "\n",
        " Relembre que neste cenário que as entradas são matrizes 28x28 de pixels e que o maior valor que um pixel pode assumir é 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOcT3Br0jpKz"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train/255, X_test/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDNOqheH82ti"
      },
      "source": [
        "## Definição do modelo\n",
        "\n",
        "Vamos construir uma Rede Neural Convolucional simples, lembrando que:\n",
        "\n",
        "- A dimensão da entrada é 28x28x1.\n",
        "- A Rede convolucional possui estrutura sequencial.\n",
        "- O problema é relativamente simples, pois não há informações de cores.  \n",
        "\n",
        "Essa rede neural terá as seguintes camadas:\n",
        "\n",
        "1. Camada convolucional 2D com 32 neurônios, função relu e filtros (3,3)\n",
        "2. Camada MaxPooling 2D com filtros 2x2\n",
        "3. Camada Flatten\n",
        "4. Rede Neural MLP com 2 camadas para classificar as características extraídas.\n",
        "  - Camanda Densa com 100 neurônios e função de ativação ReLU.\n",
        "  -  Camanda Densa com 1 neurônio para cada classe do problema e função de ativação softmax.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlIVeG1v8I9g"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (32, 32, 3)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
        "model.add(keras.layers.MaxPool2D((2, 2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(100, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(metrics=['accuracy'], loss='categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqB3Q_PIVD8T"
      },
      "source": [
        "## Visualize a quantidade de parâmetros que o modelo possui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVcLsxn-SshM"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfjfTOevVKtQ"
      },
      "source": [
        "## Treinamento do Modelo\n",
        "\n",
        "Efetue o treinamento do modelo com os seguintes parâmetros:\n",
        "\n",
        "- Épocas: 20\n",
        "- batch_size = 32\n",
        "- validation_split = 0.1\n",
        "- verbose = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "QJl5qCQyQuiG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1407/1407 [==============================] - 22s 16ms/step - loss: 0.3541 - accuracy: 0.8807 - val_loss: 1.5319 - val_accuracy: 0.6396\n",
            "Epoch 2/10\n",
            " 139/1407 [=>............................] - ETA: 19s - loss: 0.2601 - accuracy: 0.9123"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, BATCH_SIZE, EPOCHS, 1, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tesWTSS4VcNE"
      },
      "source": [
        "## Visualize os gráficos de treinamento do modelo\n",
        "\n",
        "- Perda (loss)\n",
        "- Acurácia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'loss': [0.35414639115333557],\n",
              " 'accuracy': [0.8807111382484436],\n",
              " 'val_loss': [1.5318982601165771],\n",
              " 'val_accuracy': [0.6395999789237976]}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "jxU_3KEwRQeT"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(model.history.history['loss'], color='b')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKpAh8RrRxue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDHNtRayVjUX"
      },
      "source": [
        "## Análise de Desempenho\n",
        "\n",
        "Efetue a previsão do modelo para o conjunto de testes e calcule as seguintes métricas\n",
        "\n",
        "- Acurácia\n",
        "- F1-Score\n",
        "\n",
        "Visualize também a matriz de confusão para as previsões efetuadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWnMUaEnSqXe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th9UWgPxUHDQ"
      },
      "outputs": [],
      "source": [
        "y_prev = model.predict(X_test)\n",
        "y_prev = np.argmax(y_prev, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_prev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6EJnHkWUOuV"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, y_prev, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM_uFy0nYy8C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDs2KHlXYzrq"
      },
      "source": [
        "## Previsões com a MobileNet\n",
        "\n",
        "A MobileNet é uma arquitetura de rede neural convolucional (CNN) projetada para aplicações de visão computacional em dispositivos móveis e com recursos limitados de computação. Ela foi desenvolvida pelo Google em 2017 para atender à crescente demanda por modelos de aprendizado profundo eficientes em termos de computação e recursos para dispositivos móveis, como smartphones e tablets.\n",
        "\n",
        "O principal objetivo do MobileNet é oferecer desempenho suficiente para tarefas de visão computacional, como detecção de objetos e reconhecimento de imagem, enquanto mantém uma arquitetura leve e eficiente.\n",
        "\n",
        "Consulte: https://keras.io/api/applications/mobilenet/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQLbbr59ZBzN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
