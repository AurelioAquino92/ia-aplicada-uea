{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJtcr0Xkv6Fo"
   },
   "source": [
    "# Atividade Prática 2.1 - Idade do Abalone\n",
    "\n",
    "* Disciplina _Inteligência Artificial Aplicada_\n",
    "* Professora: Elloá B. Guedes (ebgcosta@uea.edu.br)\n",
    "* Data de apresentação: 13 de janeiro de 2023\n",
    "* Data limite de entrega: 25 de janeiro de 2023\n",
    "\n",
    "\n",
    "## Equipe\n",
    "* Integrante 1: *Aurelio Aquino*\n",
    "* Integrante 2: *Jailson Bina*\n",
    "* Integrante 3: *Sthephany Costa*\n",
    "* Integrante 4: *Erica Veras*\n",
    "* Integrante 5: *Michelle de Carvalho*\n",
    "* Integrante 6: *Guilherme Rodamilans*\n",
    "* Integrante 7: *Fabiano Dolzanes*\n",
    "\n",
    "\n",
    "O Abalone é um gênero (_Haliotis_) de um moluscos gastrópodes marinhos da família _Haliotidae_. Foi identificado por Linnaeus em 1758 e suas diversas espécies podem ser encontradas em águas costeiras de quase todo o mundo. É usado na indústria alimentícia e em itens decorativos, tais como jóias ou instrumentos musicais [1](https://pt.wikipedia.org/wiki/Abalone). A idade do abalone pode ser obtida diretamente a partir de medidas físicas, porém é necessário cortar a concha, efetuar um processo de pigmentação, e então contar o número de anéis por meio de um microscópio -- tarefa considerada monótona e custosa [2](https://archive.ics.uci.edu/ml/datasets/Abalone).\n",
    "\n",
    "Outras medidas do Abalone, entretanto, são mais fáceis de obter, não danificam a concha e podem ser utilizadas para estimar a idade com um modelo inteligente por meio de um processo de Aprendizado Supervisionado. Nesta Atividade Avaliativa de caráter prático, almeja-se a proposição e avaliação de múltiplas Redes Neurais Artificiais (RNAs) do tipo _Feedforward Multilayer Perceptron_ (MLP) para o problema da classificação multi-classe da idade do abalone a partir de atributos preditores.\n",
    "\n",
    "Base de dados original: https://archive.ics.uci.edu/ml/datasets/Abalone  \n",
    "Base de dados preparada: abalone.csv (Disponível no Google Classroom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquecimento\n",
    "\n",
    "1. Abrir a base de dados\n",
    "2. Separar os atributos preditores (X) e o atributo-alvo (y) nas respectivas variáveis\n",
    "3. Imprimir a dimensão da base de dados (quantidade de exemplos, quantidade de atributos preditores)\n",
    "4. Efetue uma partição holdout 70/30 com o sklearn, distribuindo os exemplos de maneira aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4176 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0      0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "1      0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "2      0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "3      0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "4      0.425     0.300   0.095        0.3515          0.1410          0.0775   \n",
       "...      ...       ...     ...           ...             ...             ...   \n",
       "4171   0.565     0.450   0.165        0.8870          0.3700          0.2390   \n",
       "4172   0.590     0.440   0.135        0.9660          0.4390          0.2145   \n",
       "4173   0.600     0.475   0.205        1.1760          0.5255          0.2875   \n",
       "4174   0.625     0.485   0.150        1.0945          0.5310          0.2610   \n",
       "4175   0.710     0.555   0.195        1.9485          0.9455          0.3765   \n",
       "\n",
       "      Shell weight  Age  F  I  M  \n",
       "0           0.0700    7  0  0  1  \n",
       "1           0.2100    9  1  0  0  \n",
       "2           0.1550   10  0  0  1  \n",
       "3           0.0550    7  0  1  0  \n",
       "4           0.1200    8  0  1  0  \n",
       "...            ...  ... .. .. ..  \n",
       "4171        0.2490   11  1  0  0  \n",
       "4172        0.2605   10  0  0  1  \n",
       "4173        0.3080    9  0  0  1  \n",
       "4174        0.2960   10  1  0  0  \n",
       "4175        0.4950   12  0  0  1  \n",
       "\n",
       "[4176 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('abalone.csv') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Age'])\n",
    "Y = df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização dos Atributos Preditores\n",
    "\n",
    "O treinamento de uma RNA MLP é mais eficiente quando os valores que lhes são fornecidos como entrada são pequenos, pois isto favorece a convergência. Isto é feito por meio do escalonamento dos atributos preditores para o intervalo [0,1], mas precisa ser feito de maneira cautelosa, para que informações do conjunto de teste não sejam fornecidas no treinamento.\n",
    "\n",
    "Há duas estratégias para tal escalonamento: normalização e padronização. Ambas possuem características particulares, vantagens e limitações, como é possível ver aqui: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "\n",
    "No nosso caso, vamos usar a normalização. Assim, com os atributos preditores do treinamento, isto é, X_train, deve-se efetuar as seguintes operações\n",
    "\n",
    "X_train_norm = (X_train - min(X_train))/(max(X_train) - min(X_train))\n",
    "\n",
    "Em seguida, o mesmo deve ser feito com os atributos preditores do conjunto de testes, mas com padronização relativa ao conjunto de treinamento:\n",
    "\n",
    "X_test_norm = (X_test - min(X_train)))/(max(X_train) - min(X_train))\n",
    "\n",
    "Se todo o conjunto X for utilizado no escalonamento, a rede neural receberá informações do conjunto de teste por meio dos valores mínimo e máximo utilizados para preparar os dados de treinamento, o que não é desejável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - X_train.min(axis=0))/(X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_test_norm = (X_test - X_test.min(axis=0))/(X_test.max(axis=0) - X_test.min(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando a primeira RNA MLP para o Abalone\n",
    "\n",
    "1. Treine uma RNA MLP Classificadora para este problema com uma única camada e dez neurônios  \n",
    "    1.1 Utilize a função de ativação ReLU  \n",
    "    1.2 Utilize o solver Adam    \n",
    "    1.3 Imprima o passo a passo do treinamento    \n",
    "    1.4 Utilize o número máximo de épocas igual a 300  \n",
    "2. Imprima um gráfico com a perda da RNA MLP ao longo do treinamento  \n",
    "    2.1 Houve Early Stopping?  \n",
    "3. Com o modelo em questão, após o treinamento, apresente:  \n",
    "    3.1 Matriz de confusão para o conjunto de teste  \n",
    "    3.2 Acurácia  \n",
    "    3.3 F-Score  \n",
    "    3.4 Precisão  \n",
    "    3.5 Revocação  \n",
    "    \n",
    "No tocante ao Passo 3, construa funções para esta tarefa, pois serão recorrentemente utilizadas ao longo do trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.17054990\n",
      "Iteration 2, loss = 3.12126827\n",
      "Iteration 3, loss = 3.06834575\n",
      "Iteration 4, loss = 3.00766941\n",
      "Iteration 5, loss = 2.93944790\n",
      "Iteration 6, loss = 2.86677871\n",
      "Iteration 7, loss = 2.79224906\n",
      "Iteration 8, loss = 2.72129993\n",
      "Iteration 9, loss = 2.65408398\n",
      "Iteration 10, loss = 2.59325416\n",
      "Iteration 11, loss = 2.53949924\n",
      "Iteration 12, loss = 2.49653407\n",
      "Iteration 13, loss = 2.46231504\n",
      "Iteration 14, loss = 2.43370926\n",
      "Iteration 15, loss = 2.40952745\n",
      "Iteration 16, loss = 2.38801716\n",
      "Iteration 17, loss = 2.37004346\n",
      "Iteration 18, loss = 2.35371732\n",
      "Iteration 19, loss = 2.33952761\n",
      "Iteration 20, loss = 2.32705298\n",
      "Iteration 21, loss = 2.31588706\n",
      "Iteration 22, loss = 2.30577383\n",
      "Iteration 23, loss = 2.29689099\n",
      "Iteration 24, loss = 2.28890205\n",
      "Iteration 25, loss = 2.28155913\n",
      "Iteration 26, loss = 2.27470727\n",
      "Iteration 27, loss = 2.26915898\n",
      "Iteration 28, loss = 2.26318878\n",
      "Iteration 29, loss = 2.25807951\n",
      "Iteration 30, loss = 2.25278499\n",
      "Iteration 31, loss = 2.24812409\n",
      "Iteration 32, loss = 2.24367591\n",
      "Iteration 33, loss = 2.23968394\n",
      "Iteration 34, loss = 2.23544008\n",
      "Iteration 35, loss = 2.23155831\n",
      "Iteration 36, loss = 2.22808236\n",
      "Iteration 37, loss = 2.22461277\n",
      "Iteration 38, loss = 2.22075097\n",
      "Iteration 39, loss = 2.21759426\n",
      "Iteration 40, loss = 2.21458398\n",
      "Iteration 41, loss = 2.21124872\n",
      "Iteration 42, loss = 2.20849762\n",
      "Iteration 43, loss = 2.20534692\n",
      "Iteration 44, loss = 2.20250097\n",
      "Iteration 45, loss = 2.19975245\n",
      "Iteration 46, loss = 2.19735930\n",
      "Iteration 47, loss = 2.19463191\n",
      "Iteration 48, loss = 2.19218362\n",
      "Iteration 49, loss = 2.18964672\n",
      "Iteration 50, loss = 2.18698653\n",
      "Iteration 51, loss = 2.18479869\n",
      "Iteration 52, loss = 2.18287827\n",
      "Iteration 53, loss = 2.18024163\n",
      "Iteration 54, loss = 2.17810964\n",
      "Iteration 55, loss = 2.17619142\n",
      "Iteration 56, loss = 2.17415492\n",
      "Iteration 57, loss = 2.17210445\n",
      "Iteration 58, loss = 2.17023477\n",
      "Iteration 59, loss = 2.16791959\n",
      "Iteration 60, loss = 2.16613092\n",
      "Iteration 61, loss = 2.16412989\n",
      "Iteration 62, loss = 2.16242926\n",
      "Iteration 63, loss = 2.16073374\n",
      "Iteration 64, loss = 2.15883092\n",
      "Iteration 65, loss = 2.15710337\n",
      "Iteration 66, loss = 2.15548275\n",
      "Iteration 67, loss = 2.15371523\n",
      "Iteration 68, loss = 2.15235155\n",
      "Iteration 69, loss = 2.15059067\n",
      "Iteration 70, loss = 2.14910318\n",
      "Iteration 71, loss = 2.14753761\n",
      "Iteration 72, loss = 2.14580592\n",
      "Iteration 73, loss = 2.14511007\n",
      "Iteration 74, loss = 2.14313650\n",
      "Iteration 75, loss = 2.14166107\n",
      "Iteration 76, loss = 2.14031042\n",
      "Iteration 77, loss = 2.13892966\n",
      "Iteration 78, loss = 2.13731098\n",
      "Iteration 79, loss = 2.13590232\n",
      "Iteration 80, loss = 2.13452691\n",
      "Iteration 81, loss = 2.13335794\n",
      "Iteration 82, loss = 2.13218350\n",
      "Iteration 83, loss = 2.13098959\n",
      "Iteration 84, loss = 2.12979911\n",
      "Iteration 85, loss = 2.12805144\n",
      "Iteration 86, loss = 2.12694134\n",
      "Iteration 87, loss = 2.12569715\n",
      "Iteration 88, loss = 2.12481621\n",
      "Iteration 89, loss = 2.12336754\n",
      "Iteration 90, loss = 2.12243299\n",
      "Iteration 91, loss = 2.12112479\n",
      "Iteration 92, loss = 2.11973063\n",
      "Iteration 93, loss = 2.11871405\n",
      "Iteration 94, loss = 2.11752540\n",
      "Iteration 95, loss = 2.11644854\n",
      "Iteration 96, loss = 2.11531800\n",
      "Iteration 97, loss = 2.11450403\n",
      "Iteration 98, loss = 2.11292809\n",
      "Iteration 99, loss = 2.11193371\n",
      "Iteration 100, loss = 2.11095092\n",
      "Iteration 101, loss = 2.11030198\n",
      "Iteration 102, loss = 2.10898182\n",
      "Iteration 103, loss = 2.10820640\n",
      "Iteration 104, loss = 2.10716299\n",
      "Iteration 105, loss = 2.10581803\n",
      "Iteration 106, loss = 2.10467859\n",
      "Iteration 107, loss = 2.10381718\n",
      "Iteration 108, loss = 2.10287632\n",
      "Iteration 109, loss = 2.10218711\n",
      "Iteration 110, loss = 2.10117465\n",
      "Iteration 111, loss = 2.09994432\n",
      "Iteration 112, loss = 2.09914409\n",
      "Iteration 113, loss = 2.09811303\n",
      "Iteration 114, loss = 2.09735029\n",
      "Iteration 115, loss = 2.09633715\n",
      "Iteration 116, loss = 2.09534819\n",
      "Iteration 117, loss = 2.09430282\n",
      "Iteration 118, loss = 2.09361470\n",
      "Iteration 119, loss = 2.09246840\n",
      "Iteration 120, loss = 2.09172292\n",
      "Iteration 121, loss = 2.09091125\n",
      "Iteration 122, loss = 2.09010261\n",
      "Iteration 123, loss = 2.08931393\n",
      "Iteration 124, loss = 2.08824495\n",
      "Iteration 125, loss = 2.08730404\n",
      "Iteration 126, loss = 2.08638485\n",
      "Iteration 127, loss = 2.08561760\n",
      "Iteration 128, loss = 2.08491375\n",
      "Iteration 129, loss = 2.08399137\n",
      "Iteration 130, loss = 2.08310313\n",
      "Iteration 131, loss = 2.08233496\n",
      "Iteration 132, loss = 2.08163590\n",
      "Iteration 133, loss = 2.08067499\n",
      "Iteration 134, loss = 2.07981091\n",
      "Iteration 135, loss = 2.07883642\n",
      "Iteration 136, loss = 2.07811145\n",
      "Iteration 137, loss = 2.07736971\n",
      "Iteration 138, loss = 2.07668191\n",
      "Iteration 139, loss = 2.07600559\n",
      "Iteration 140, loss = 2.07491751\n",
      "Iteration 141, loss = 2.07415405\n",
      "Iteration 142, loss = 2.07339062\n",
      "Iteration 143, loss = 2.07260057\n",
      "Iteration 144, loss = 2.07188702\n",
      "Iteration 145, loss = 2.07085429\n",
      "Iteration 146, loss = 2.07015223\n",
      "Iteration 147, loss = 2.06965958\n",
      "Iteration 148, loss = 2.06863443\n",
      "Iteration 149, loss = 2.06789590\n",
      "Iteration 150, loss = 2.06722784\n",
      "Iteration 151, loss = 2.06643389\n",
      "Iteration 152, loss = 2.06573882\n",
      "Iteration 153, loss = 2.06514052\n",
      "Iteration 154, loss = 2.06423834\n",
      "Iteration 155, loss = 2.06322221\n",
      "Iteration 156, loss = 2.06268409\n",
      "Iteration 157, loss = 2.06195292\n",
      "Iteration 158, loss = 2.06139988\n",
      "Iteration 159, loss = 2.06062216\n",
      "Iteration 160, loss = 2.06024971\n",
      "Iteration 161, loss = 2.05927151\n",
      "Iteration 162, loss = 2.05875100\n",
      "Iteration 163, loss = 2.05762631\n",
      "Iteration 164, loss = 2.05711615\n",
      "Iteration 165, loss = 2.05627352\n",
      "Iteration 166, loss = 2.05532994\n",
      "Iteration 167, loss = 2.05475979\n",
      "Iteration 168, loss = 2.05404970\n",
      "Iteration 169, loss = 2.05365120\n",
      "Iteration 170, loss = 2.05255111\n",
      "Iteration 171, loss = 2.05216376\n",
      "Iteration 172, loss = 2.05150938\n",
      "Iteration 173, loss = 2.05067070\n",
      "Iteration 174, loss = 2.04990361\n",
      "Iteration 175, loss = 2.04894589\n",
      "Iteration 176, loss = 2.04831261\n",
      "Iteration 177, loss = 2.04818045\n",
      "Iteration 178, loss = 2.04698519\n",
      "Iteration 179, loss = 2.04689172\n",
      "Iteration 180, loss = 2.04595995\n",
      "Iteration 181, loss = 2.04528617\n",
      "Iteration 182, loss = 2.04428848\n",
      "Iteration 183, loss = 2.04344956\n",
      "Iteration 184, loss = 2.04293540\n",
      "Iteration 185, loss = 2.04229972\n",
      "Iteration 186, loss = 2.04167483\n",
      "Iteration 187, loss = 2.04107811\n",
      "Iteration 188, loss = 2.04039878\n",
      "Iteration 189, loss = 2.03975198\n",
      "Iteration 190, loss = 2.03929636\n",
      "Iteration 191, loss = 2.03859113\n",
      "Iteration 192, loss = 2.03790971\n",
      "Iteration 193, loss = 2.03710722\n",
      "Iteration 194, loss = 2.03664481\n",
      "Iteration 195, loss = 2.03597544\n",
      "Iteration 196, loss = 2.03542757\n",
      "Iteration 197, loss = 2.03430412\n",
      "Iteration 198, loss = 2.03378828\n",
      "Iteration 199, loss = 2.03322339\n",
      "Iteration 200, loss = 2.03285047\n",
      "Iteration 201, loss = 2.03166417\n",
      "Iteration 202, loss = 2.03114938\n",
      "Iteration 203, loss = 2.03028606\n",
      "Iteration 204, loss = 2.02979231\n",
      "Iteration 205, loss = 2.02923945\n",
      "Iteration 206, loss = 2.02855724\n",
      "Iteration 207, loss = 2.02827935\n",
      "Iteration 208, loss = 2.02735609\n",
      "Iteration 209, loss = 2.02673827\n",
      "Iteration 210, loss = 2.02609681\n",
      "Iteration 211, loss = 2.02522884\n",
      "Iteration 212, loss = 2.02477781\n",
      "Iteration 213, loss = 2.02471882\n",
      "Iteration 214, loss = 2.02403550\n",
      "Iteration 215, loss = 2.02271316\n",
      "Iteration 216, loss = 2.02233860\n",
      "Iteration 217, loss = 2.02185778\n",
      "Iteration 218, loss = 2.02098168\n",
      "Iteration 219, loss = 2.02034999\n",
      "Iteration 220, loss = 2.01982173\n",
      "Iteration 221, loss = 2.01923169\n",
      "Iteration 222, loss = 2.01861350\n",
      "Iteration 223, loss = 2.01787766\n",
      "Iteration 224, loss = 2.01731931\n",
      "Iteration 225, loss = 2.01665024\n",
      "Iteration 226, loss = 2.01610817\n",
      "Iteration 227, loss = 2.01529693\n",
      "Iteration 228, loss = 2.01460046\n",
      "Iteration 229, loss = 2.01426007\n",
      "Iteration 230, loss = 2.01365459\n",
      "Iteration 231, loss = 2.01297025\n",
      "Iteration 232, loss = 2.01248566\n",
      "Iteration 233, loss = 2.01209190\n",
      "Iteration 234, loss = 2.01106605\n",
      "Iteration 235, loss = 2.01142365\n",
      "Iteration 236, loss = 2.01038253\n",
      "Iteration 237, loss = 2.00961924\n",
      "Iteration 238, loss = 2.00896648\n",
      "Iteration 239, loss = 2.00832319\n",
      "Iteration 240, loss = 2.00801031\n",
      "Iteration 241, loss = 2.00737647\n",
      "Iteration 242, loss = 2.00667692\n",
      "Iteration 243, loss = 2.00627087\n",
      "Iteration 244, loss = 2.00519557\n",
      "Iteration 245, loss = 2.00477795\n",
      "Iteration 246, loss = 2.00409980\n",
      "Iteration 247, loss = 2.00373182\n",
      "Iteration 248, loss = 2.00316769\n",
      "Iteration 249, loss = 2.00279195\n",
      "Iteration 250, loss = 2.00204768\n",
      "Iteration 251, loss = 2.00174363\n",
      "Iteration 252, loss = 2.00082511\n",
      "Iteration 253, loss = 2.00073200\n",
      "Iteration 254, loss = 1.99987944\n",
      "Iteration 255, loss = 1.99939473\n",
      "Iteration 256, loss = 1.99860022\n",
      "Iteration 257, loss = 1.99823257\n",
      "Iteration 258, loss = 1.99764061\n",
      "Iteration 259, loss = 1.99721360\n",
      "Iteration 260, loss = 1.99645174\n",
      "Iteration 261, loss = 1.99580041\n",
      "Iteration 262, loss = 1.99548338\n",
      "Iteration 263, loss = 1.99495651\n",
      "Iteration 264, loss = 1.99449387\n",
      "Iteration 265, loss = 1.99375458\n",
      "Iteration 266, loss = 1.99348654\n",
      "Iteration 267, loss = 1.99308688\n",
      "Iteration 268, loss = 1.99224970\n",
      "Iteration 269, loss = 1.99188196\n",
      "Iteration 270, loss = 1.99155461\n",
      "Iteration 271, loss = 1.99066876\n",
      "Iteration 272, loss = 1.99018665\n",
      "Iteration 273, loss = 1.98960405\n",
      "Iteration 274, loss = 1.98936552\n",
      "Iteration 275, loss = 1.98871639\n",
      "Iteration 276, loss = 1.98835810\n",
      "Iteration 277, loss = 1.98832819\n",
      "Iteration 278, loss = 1.98705818\n",
      "Iteration 279, loss = 1.98668951\n",
      "Iteration 280, loss = 1.98624067\n",
      "Iteration 281, loss = 1.98574043\n",
      "Iteration 282, loss = 1.98533661\n",
      "Iteration 283, loss = 1.98438126\n",
      "Iteration 284, loss = 1.98431289\n",
      "Iteration 285, loss = 1.98347968\n",
      "Iteration 286, loss = 1.98308488\n",
      "Iteration 287, loss = 1.98276054\n",
      "Iteration 288, loss = 1.98244446\n",
      "Iteration 289, loss = 1.98200992\n",
      "Iteration 290, loss = 1.98127062\n",
      "Iteration 291, loss = 1.98077014\n",
      "Iteration 292, loss = 1.98053373\n",
      "Iteration 293, loss = 1.98013718\n",
      "Iteration 294, loss = 1.97947475\n",
      "Iteration 295, loss = 1.97919156\n",
      "Iteration 296, loss = 1.97872066\n",
      "Iteration 297, loss = 1.97809221\n",
      "Iteration 298, loss = 1.97778682\n",
      "Iteration 299, loss = 1.97760766\n",
      "Iteration 300, loss = 1.97669166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mnsaaqui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10,), max_iter=300, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(10,), max_iter=300, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10,), max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede = MLPClassifier((10,), activation='relu', solver='adam', verbose=True, max_iter=300)\n",
    "rede.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(Y_test, rede.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimando o número de neurônios\n",
    "\n",
    "Um dos problemas de pesquisa com redes neurais artificiais consiste na determinação do número de neurônios em sua arquitetura. Embora não seja possível definir a priori qual rede neural é adequada para um problema, pois isto só é possível mediante uma busca exaustiva, há regras na literatura que sugerem o número de neurônios escondidos, tal como a regra da Pirâmide Geométrica, dada a seguir:\n",
    "\n",
    "$$N_h = \\alpha \\cdot \\sqrt{N_i \\cdot N_o},$$\n",
    "\n",
    "em que $N_h$ é o número de neurônios ocultos (a serem distribuídos em uma ou duas camadas ocultas), $N_i$ é o número de neurônios na camada de entrada e $N_o$ é o número de neurônios na camada de saída. \n",
    "\n",
    "1. Consulte a documentação da classe MLPClassifier (disponível em https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) e obtenha os valores de $N_i$ e $N_o$.\n",
    "2. Teste os valores de $\\alpha$ como sendo iguais a $0.5$, $2$ e $3$.\n",
    "3. Proponha pelo menos 30 arquiteturas de neurônios para RNAS MLPs segundo a regra da pirâmide geométrica  \n",
    "    3.1 Ao final desta etapa, deve-se obter uma lista contendo 30 elementos do tipo 2-tupla  \n",
    "    3.2 Obtenha as arquiteturas usando laços, listas, tuplas, etc. Soluções _hard-coded_ são desencorajadas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca em Grade\n",
    "\n",
    "Uma maneira padrão de escolher os parâmetros de um modelo de _Machine Learning_ é por meio de uma busca em grade via força bruta. O algoritmo da busca em grade é dado como segue:\n",
    "\n",
    "1. Escolha a métrica de desempenho que você deseja maximizar  \n",
    "2. Escolha o algoritmo de Machine Learning (exemplo: MLPClassifier). Em seguida, defina os parâmetros ou hiperparâmetros deste tipo de modelo sobre os quais você dseja otimizar (número de épocas, taxa de aprendizado, etc.) e construa um array de valores a serem testados para cada parâmetro ou hiperparâmetro.  \n",
    "3. Defina a grade de busca, a qual é dada como o produto cartesiano de cada parâmetro a ser testado. Por exemplo, para os arrays [50, 100, 1000] e [10, 15], tem-se que a grade é [(50,10), (50,15), (100,10), (100,15), (1000,10), (1000,15)].\n",
    "4. Para cada combinação de parâmetros a serem otimizados, utilize o conjunto de treinamento para realizar uma validação cruzada (holdout ou k-fold) e calcule a métrica de avaliação no conjunto de teste (ou conjuntos de teste)\n",
    "5. Escolha a combinação de parâmetros que maximizam a métrica de avaliação. Este é o modelo otimizado.\n",
    "\n",
    "Por que esta abordagem funciona? Porque a busca em grade efetua uma pesquisa extensiva sobre as possíveis combinações de valores para cada um dos parâmetros a serem ajustados. Para cada combinação, ela estima a performance do modelo em dados novos. Por fim, o modelo com melhor métrica de desempenho é escolhido. Tem-se então que este modelo é o que melhor pode vir a generalizar mediante dados nunca antes vistos.\n",
    "\n",
    "Sua busca em grade deve considerar:\n",
    "\n",
    "1. Validação Cruzada Holdout 70/30 com normalização, como definido anteriormente, com aferição de desempenho no conjunto de testes\n",
    "2. Parâmetros:  \n",
    "  2.1 30 arquiteturas propostas para o número de neurônios ocultos no item anterior  \n",
    "  2.2 Funções de ativação (ReLU e Sigmóide)\n",
    "3. Hiperparâmetros:  \n",
    "  3.1 Batch_size: 16 ou 32  \n",
    "  3.2 Solver: Adam  \n",
    "  3.3 $\\beta_1$: 1, 0.9, 0.8  \n",
    "  3.4 $\\beta_1$: 0.999, 0.95, 0.9  \n",
    "  3.5 Paciência (n_iter_no_change): 25 ou 50    \n",
    "4. Nesta busca em grande, contemple a utilização do objeto [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "5. Apresente as três propostas com melhor desempenho na busca em grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização por Ajuste Fino (Fine-Tuning)\n",
    "\n",
    "Considerando a etapa anterior, foram identificadas 3 melhores RNAs MLPs com seus parâmetros e hiperparâmetros para o problema da classificação multi-classe da idade do Abalone. Uma das questões remanescentes é se o número de épocas foi suficiente para o treinamento e melhor aprendizado das características do problema, mas lembrando-se de prevenir overfitting.\n",
    "\n",
    "Com essas Top-3 RNAs identificadas, faça o que se pede:\n",
    "1. Aumente o número de épocas do treinamento para 600\n",
    "2. Aumenta a paciência para 60 (10% das épocas do treinamento)\n",
    "\n",
    "Repita o treinamento e o teste das 3 melhores RNAs e verifique se houve melhoria de desempenho. Apresente detalhadamente, para cada uma das redes, as métricas de acurácia, precisão, revocação e F1-Score (weighted), bem como a matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4pMGRAev6Fr"
   },
   "source": [
    "## Validação Cruzada k-fold\n",
    "\n",
    "Na elaboração da busca em grade e com sua posterior otimização, fomos capazes de identificar as três melhores arquiteturas para o problema. O passo seguinte consiste em avaliar a robustez da RNA MLP com melhor desempenho. Caso os valores de desempenho tenham sido muito próximos, assuma que é a melhor rede é a que possui menos parâmetros, isto é, a menor quantidade de pesos.\n",
    "\n",
    "Nessa etapa, vamos utilizar uma estratégia de validação cruzada ainda não explorada até o momento: a validação cruzada k-fold. Segundo a mesma, o conjunto de dados é particionado em k partes: a cada iteração, separa-se uma das partes para teste e o modelo é treinado com as k-1 partes remanescentes. Valores sugestivos de k na literatura são k = 3, 5 ou 10, pois o custo computacional desta validação dos modelos é alto. A métrica de desempenho é resultante da média dos desempenhos nas k iterações. A figura a seguir ilustra a ideia desta avaliação\n",
    "\n",
    "<img src = \"https://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png\" width=600></img>\n",
    "\n",
    "Considerando a métrica de desempenho F1-Score, avalie a melhor RNA MLP no tocante ao seu desempenho em uma validação cruzada $10$-fold. Consulte: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html. Apresente claramente os resultados obtidos desta validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVaGXTaUVAmg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFgIIIovv6Fu"
   },
   "source": [
    "## Empacotando a solução\n",
    "\n",
    "Suponha que você deva entregar este classificador ao órgão responsável pelo manejo dos abalones em uma determinada região. Para tanto, você deve fazer uma preparação do mesmo para utilização neste cenário. Uma vez que já identificou os melhores parâmetros e hiperparâmetros, o passo remanescente consiste em treinar o modelo com estes valores e todos os dados disponíveis, salvando o conjunto de pesos do modelo ao final para entrega ao cliente. Assim, finalize o projeto prático realizando tais passos.\n",
    "\n",
    "1. Consulte a documentação a seguir:\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html  \n",
    "2. Treine o modelo com todos os dados  \n",
    "3. Salve o modelo em disco  \n",
    "4. Construa uma rotina que recupere o modelo em disco  \n",
    "5. Mostre que a rotina é funcional, fazendo previsões com todos os elementos do dataset e exibindo uma matriz de confusão das mesmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9o1T7pzVAmn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "PP2.2.3 - Validação Cruzada e Busca em Grade.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
